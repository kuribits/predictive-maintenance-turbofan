{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turbofan POC Part 1: Dataloading\n",
    "CAA 20/07/2020\n",
    "\n",
    "In this notebook, we will use PyGrid and PySyft to train a model with differential privacy and multi-party computation using a federated approach.\n",
    "\n",
    "Dependencies for this notebook:\n",
    "- miniconda3 or anaconda3 (for environment management)\n",
    "- Python >= 3.7\n",
    "- PySyft 0.2.7\n",
    "\n",
    "NOTE: Before running this notebook, ensure that you have run `bash prep_turbofan.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Run PyGridNetwork and PyGridNode in the background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we distribute Turbofan data from a local directory to 2 workers. Following the above section, we will make sure to run the following process at the following addresses:\n",
    "- PyGridNetwork: server Bob (http://localhost:5000)\n",
    "- PyGridNode: server Bob (http://localhost:3000)\n",
    "- PyGridNode: server Alice (http://18.218.13.132:3001)\n",
    "- This Jupyter Notebook: server Bob (http://localhost:8000)â€”you should be able to run this notebook on any server which is running a PyGridNetwork, or PyGridNode associated with the PyGridNetwork."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For allowing communication *between* training workers and any coordinating servers, we run PyGridNetwork on a server of choice as follows:\n",
    "\n",
    "1. Clone [PyGridNetwork](https://github.com/OpenMined/PyGridNetwork)\n",
    "1. Descend into cloned PyGridNetwork directory\n",
    "1. Create and activate `conda` environment (can be shared by PyGridNetwork and PyGridNode)\n",
    "1. Install dependencies: `pip install openmined.gridnetwork`\n",
    "1. Run PyGridNetwork: `python -m gridnetwork --port DESIRED_PORT --start_local_db `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For allowing workers to communicate with the PyGridNetwork process, start the desired number of PyGridNodes (equal to number of desired workers) per server. The following steps should be taken per desired worker:\n",
    "\n",
    "1. Clone [PyGridNode](https://github.com/OpenMined/PyGridNode)\n",
    "1. Descend into cloned PyGridNode directory\n",
    "1. Create and activate `conda` environment (can be shared by PyGridNetwork and PyGridNode)\n",
    "1. Install dependencies: `pip install .`\n",
    "1. Run PyGridNode: `python -m gridnode --id alice --port DESIRED_PORT --host SERVER_IPV4_ADDRESS --gateway_url HTTPS_URL_OF_PYGRIDNETWORK_SERVER`\n",
    "\n",
    "(NOTE: PyGridNode will be deprecated with its function moved to the PySyft library). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Populate nodes with data\n",
    "It is possible to use a coordinating server to distribute data to the nodes, either from a remote source or from a local directory.\n",
    "\n",
    "IMPORTANT! Before running this section, make sure to clone the [OpenMined Turbofan POC](https://github.com/matthiaslau/Turbofan-Federated-Learning-POC) repository, and follow instructions for downloading and preprocessing the dataset.\n",
    "\n",
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.clients.dynamic_fl_client import DynamicFLClient\n",
    "import torch\n",
    "import pandas as pd\n",
    "from numpy.random import laplace\n",
    "from math import floor\n",
    "\n",
    "from federated_trainer.helper.data_helper import _load_data, WINDOW_SIZE, _drop_unnecessary_columns, _transform_to_windowed_data, get_data_loader, _clip_rul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rul_to_train_data(train_data):\n",
    "    \"\"\" Calculate and add the RUL to all rows in the given training data.\n",
    "\n",
    "    :param train_data: The training data\n",
    "    :return: The training data with added RULs\n",
    "    \"\"\"\n",
    "    # retrieve the max cycles per engine_node: RUL\n",
    "    train_rul = pd.DataFrame(train_data.groupby('engine_no')['time_in_cycles'].max()).reset_index()\n",
    "\n",
    "    # merge the RULs into the training data\n",
    "    train_rul.columns = ['engine_no', 'max']\n",
    "    train_data = train_data.merge(train_rul, on=['engine_no'], how='left')\n",
    "\n",
    "    # add the current RUL for every cycle\n",
    "    train_data['RUL'] = train_data['max'] - train_data['time_in_cycles']\n",
    "    train_data.drop('max', axis=1, inplace=True)\n",
    "\n",
    "    return train_data\n",
    "\n",
    "def round_to_multiple(x, base):\n",
    "    '''\n",
    "    Round x down to multiple of base\n",
    "    '''\n",
    "    return base * floor(x/base)\n",
    "\n",
    "def batch(tensor, batch_size):\n",
    "    features_size = tensor.shape[1:]\n",
    "    # shuffle and batch\n",
    "    randi = torch.randperm(tensor.shape[0])\n",
    "    # remove undersized tensor\n",
    "    out = tensor[randi].split(batch_size)[:-1]\n",
    "    out = torch.cat(out).view(-1, batch_size, *features_size)\n",
    "    return out\n",
    "\n",
    "def tuple_batch(tensors, batch_size):\n",
    "    '''\n",
    "    tensors: tuple of tensors\n",
    "    '''\n",
    "    return (batch(t, batch_size) for t in tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\"\n",
    "DATA_NAME = \"train_data_worker_1.txt\"\n",
    "MINIBATCH_SIZE = 4\n",
    "NOISE = 0.2\n",
    "DP_TYPE = 'local'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hook Torch\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "nodes = [\"ws://18.221.43.195:3000/\",\n",
    "         \"ws://18.221.43.195:3001/\",]\n",
    "\n",
    "compute_nodes = []\n",
    "for node in nodes:\n",
    "    compute_nodes.append(DynamicFLClient(hook, node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "The code below will load prepared data from the Turbofan demo repository after running `data_preprocess.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "11422 features with shape (80, 11)\n11422 labels with shape (11422, 1)\n"
    }
   ],
   "source": [
    "data = _load_data(DATA_NAME, DATA_PATH)\n",
    "data_dropcol = _drop_unnecessary_columns(data)\n",
    "data_rul = add_rul_to_train_data(data_dropcol)\n",
    "x, y = _transform_to_windowed_data(data_rul, WINDOW_SIZE)\n",
    "y = _clip_rul(y)\n",
    " # transform to torch tensor\n",
    "tensor_x = torch.Tensor(x)\n",
    "tensor_y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Add differential privacy to data\n",
    "We can add noise to the data at this point if we want to simulate the addition of noise by distributed data owners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def laplacian_mechanism(input_tensor, sensitivity=0.5, epsilon=0.05):\n",
    "    '''\n",
    "    sensitivity and epsilon are arbitrarily \n",
    "    chosen for now\n",
    "    '''\n",
    "    beta = sensitivity / epsilon\n",
    "    noise = torch.tensor(laplace(0, beta, 1))\n",
    "    return input_tensor + noise\n",
    "\n",
    "def add_noise(input_tensor, p_noise):\n",
    "    '''\n",
    "    tensor: input tensor\n",
    "    p_noise: probability with which noise is added\n",
    "    '''\n",
    "    be_honest = (torch.rand(input_tensor.shape) < p_noise).float()\n",
    "    tensor_artificial = laplacian_mechanism(input_tensor)\n",
    "    # add noise\n",
    "    mod_tensor = input_tensor.float() * be_honest + (1 - be_honest) * tensor_artificial\n",
    "    sk_tensor = mod_tensor.float().mean()\n",
    "    # de-skew result\n",
    "    noisy_tensor = ((mod_tensor / p_noise) - 0.5) * p_noise / (1 - p_noise)\n",
    "    return mod_tensor.type(torch.float32)\n",
    "\n",
    "if DP_TYPE=='local':\n",
    "    tensor_x = add_noise(tensor_x, NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "trainloader = torch.utils.data.DataLoader(dataset_train, \n",
    "    # split data equally among nodes with shuffle\n",
    "    batch_size=dataset_train.__len__()//len(compute_nodes),\n",
    "    shuffle=True,\n",
    "    drop_last=True,)\n",
    "    #pin_memory=True) for faster dataloading to CUDA\n",
    "dataiter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag and send split datasets to each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1427, 4, 80, 11]) torch.Size([1427, 4, 1])\ntorch.Size([1427, 4, 80, 11]) torch.Size([1427, 4, 1])\n"
    }
   ],
   "source": [
    "\n",
    "shared_x = []\n",
    "shared_y = []\n",
    "for node in compute_nodes:\n",
    "    # create minibatches\n",
    "    worker_batch = dataiter.next()\n",
    "    sensors_train_tfan, labels_train_tfan = tuple_batch(worker_batch, MINIBATCH_SIZE)\n",
    "    print(sensors_train_tfan.shape, labels_train_tfan.shape)\n",
    "    # Tag tensors (allows them to be retrieved later)\n",
    "    if not DP_TYPE:\n",
    "        tagged_sensors = sensors_train_tfan.tag(\"#X\", \"#turbofan\", \"#dataset\").describe(\"The input datapoints to the Turbofan dataset.\")\n",
    "    elif DP_TYPE=='local':\n",
    "        tagged_sensors = sensors_train_tfan.tag(\"#X\", \"#localdp\", \"#turbofan\", \"#dataset\").describe(\"The input datapoints to the Turbofan dataset.\")\n",
    "    tagged_label = labels_train_tfan.tag(\"#Y\", \"#turbofan\", \"#dataset\").describe(\"The input labels to the Turbofan dataset.\")\n",
    "    \n",
    "    shared_x.append(tagged_sensors.send(node))\n",
    "    shared_y.append(tagged_label.send(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X tensor pointers:  [(Wrapper)>[PointerTensor | me:46059304619 -> alice:55142619565]\n\tTags: #localdp #X #turbofan #dataset \n\tShape: torch.Size([1427, 4, 80, 11])\n\tDescription: The input datapoints to the Turbofan dataset...., (Wrapper)>[PointerTensor | me:98090336493 -> bob:86639925161]\n\tTags: #localdp #X #turbofan #dataset \n\tShape: torch.Size([1427, 4, 80, 11])\n\tDescription: The input datapoints to the Turbofan dataset....]\nY tensor pointers:  [(Wrapper)>[PointerTensor | me:25981771719 -> alice:38608061807]\n\tTags: #Y #turbofan #dataset \n\tShape: torch.Size([1427, 4, 1])\n\tDescription: The input labels to the Turbofan dataset...., (Wrapper)>[PointerTensor | me:65285311092 -> bob:3369699187]\n\tTags: #Y #turbofan #dataset \n\tShape: torch.Size([1427, 4, 1])\n\tDescription: The input labels to the Turbofan dataset....]\n"
    }
   ],
   "source": [
    "# print(\"X tensor pointers: \", shared_x1, shared_x2)\n",
    "# print(\"Y tensor pointers: \", shared_y1, shared_y2)\n",
    "\n",
    "print(\"X tensor pointers: \", shared_x)\n",
    "print(\"Y tensor pointers: \", shared_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disconnect nodes\n",
    "\n",
    "To ensure that our training process (in the Part 2 notebook), if located on the same server, is not using cached or local data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in compute_nodes:\n",
    "    node.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('syft': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596064016799"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}