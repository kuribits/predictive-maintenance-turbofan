{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turbofan POC: Populate nodes with data\n",
    "CAA 04/08/2020\n",
    "\n",
    "In this notebook, we will use use a coordinating server to distribute data to the nodes, either from a remote source or from a local directory.\n",
    "\n",
    "NOTE: Before running this notebook, ensure that you have run `bash init.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.clients.dynamic_fl_client import DynamicFLClient\n",
    "import torch\n",
    "import pandas as pd\n",
    "from numpy.random import laplace\n",
    "from math import floor\n",
    "\n",
    "from turbofanpoc.federated_trainer.helper.data_helper import _load_data, WINDOW_SIZE, _drop_unnecessary_columns, _transform_to_windowed_data, get_data_loader, _clip_rul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rul_to_train_data(train_data):\n",
    "    \"\"\" Calculate and add the RUL to all rows in the given training data.\n",
    "\n",
    "    :param train_data: The training data\n",
    "    :return: The training data with added RULs\n",
    "    \"\"\"\n",
    "    # retrieve the max cycles per engine_node: RUL\n",
    "    train_rul = pd.DataFrame(train_data.groupby('engine_no')['time_in_cycles'].max()).reset_index()\n",
    "\n",
    "    # merge the RULs into the training data\n",
    "    train_rul.columns = ['engine_no', 'max']\n",
    "    train_data = train_data.merge(train_rul, on=['engine_no'], how='left')\n",
    "\n",
    "    # add the current RUL for every cycle\n",
    "    train_data['RUL'] = train_data['max'] - train_data['time_in_cycles']\n",
    "    train_data.drop('max', axis=1, inplace=True)\n",
    "\n",
    "    return train_data\n",
    "\n",
    "def round_to_multiple(x, base):\n",
    "    '''\n",
    "    Round x down to multiple of base\n",
    "    '''\n",
    "    return base * floor(x/base)\n",
    "\n",
    "def batch(tensor, batch_size):\n",
    "    features_size = tensor.shape[1:]\n",
    "    # shuffle and batch\n",
    "    randi = torch.randperm(tensor.shape[0])\n",
    "    # remove undersized tensor\n",
    "    out = tensor[randi].split(batch_size)[:-1]\n",
    "    out = torch.cat(out).view(-1, batch_size, *features_size)\n",
    "    return out\n",
    "\n",
    "def tuple_batch(tensors, batch_size):\n",
    "    '''\n",
    "    tensors: tuple of tensors\n",
    "    '''\n",
    "    return (batch(t, batch_size) for t in tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./turbofanpoc/data\"\n",
    "DATA_NAME = \"train_data_initial.txt\"\n",
    "MINIBATCH_SIZE = 4\n",
    "NOISE = 0.2\n",
    "DP_TYPE = 'local'\n",
    "LABEL_DISTR_SKEW = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:root:Torch was already hooked... skipping hooking process\n"
    }
   ],
   "source": [
    "# Hook Torch\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "nodes = [\"ws://18.224.229.188:3000/\",\n",
    "         \"ws://18.224.229.188:3001/\"]\n",
    "\n",
    "compute_nodes = []\n",
    "for node in nodes:\n",
    "    compute_nodes.append(DynamicFLClient(hook, node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "The code below will load prepared data from the Turbofan POC repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1245 features with shape (80, 11)\n1245 labels with shape (1245, 1)\n"
    }
   ],
   "source": [
    "data = _load_data(DATA_NAME, DATA_PATH)\n",
    "data_dropcol = _drop_unnecessary_columns(data)\n",
    "data_rul = add_rul_to_train_data(data_dropcol)\n",
    "x, y = _transform_to_windowed_data(data_rul, WINDOW_SIZE)\n",
    "y = _clip_rul(y)\n",
    " # transform to torch tensor\n",
    "tensor_x = torch.Tensor(x)\n",
    "tensor_y = torch.Tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Add differential privacy to data\n",
    "We can add noise to the data at this point if we want to simulate the addition of noise by distributed data owners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def laplacian_mechanism(input_tensor, sensitivity=0.5, epsilon=0.05):\n",
    "    '''\n",
    "    sensitivity and epsilon are arbitrarily \n",
    "    chosen for now\n",
    "    '''\n",
    "    beta = sensitivity / epsilon\n",
    "    noise = torch.tensor(laplace(0, beta, 1))\n",
    "    return input_tensor + noise\n",
    "\n",
    "def add_noise(input_tensor, p_noise):\n",
    "    '''\n",
    "    tensor: input tensor\n",
    "    p_noise: probability with which noise is added\n",
    "    '''\n",
    "    be_honest = (torch.rand(input_tensor.shape) < p_noise).float()\n",
    "    tensor_artificial = laplacian_mechanism(input_tensor)\n",
    "    # add noise\n",
    "    mod_tensor = input_tensor.float() * be_honest + (1 - be_honest) * tensor_artificial\n",
    "    sk_tensor = mod_tensor.float().mean()\n",
    "    # de-skew result\n",
    "    noisy_tensor = ((mod_tensor / p_noise) - 0.5) * p_noise / (1 - p_noise)\n",
    "    return mod_tensor.type(torch.float32)\n",
    "\n",
    "if DP_TYPE=='local':\n",
    "    tensor_x = add_noise(tensor_x, NOISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Introduce skew (non-IIDness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-23a1cbc47558>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mLABEL_DISTR_SKEW\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdataiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_distribution_skew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABEL_DISTR_SKEW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-23a1cbc47558>\u001b[0m in \u001b[0;36mlabel_distribution_skew\u001b[0;34m(x, y, partitions, skew)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mworker_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mN_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mworker_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-23a1cbc47558>\u001b[0m in \u001b[0;36mworker_split\u001b[0;34m(N_labels, N_workers)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mworker_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m\"\"\"number of labels to assign to n workers\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mworker_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_labels\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mworker_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_workers\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mworker_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworker_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft27/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mroute_method_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# means that there is a wrapper to remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft27/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "def label_distribution_skew(x, y, partitions, skew=1):\n",
    "    def worker_split(N_labels, N_workers):\n",
    "        \"\"\"number of labels to assign to n workers\"\"\"\n",
    "        worker_labels = round(max(1, N_labels / N_workers))\n",
    "        worker_split = round(max(1, N_workers / N_labels))\n",
    "        return worker_labels, worker_split\n",
    "\n",
    "    worker_data = []\n",
    "    N_labels = torch.histc(y, bins=partitions, max=500)\n",
    "    n_labels, n_workers = worker_split(N_labels, partitions)\n",
    "    \n",
    "    worker_idx = 0\n",
    "    for label_idx in range(0, N_labels, n_labels):\n",
    "        mask = np.isin(y, range(label_idx, label_idx+n_labels))\n",
    "        subset_idx = np.argwhere(mask)[:, 0]\n",
    "        n_samples = subset_idx.shape[0]\n",
    "        sample_size = math.floor(skew*n_samples)\n",
    "        subset_idx = np.random.choice(subset_idx, sample_size, replace=False)\n",
    "        x_subset = x[subset_idx, ]\n",
    "        y_subset = y[subset_idx]   \n",
    "    \n",
    "        for partition in zip(np.array_split(x_subset, n_workers),\n",
    "                             np.array_split(y_subset, n_workers)):\n",
    "            worker_data.append(partition)\n",
    "    \n",
    "        x = np.delete(x, subset_idx, axis=0)\n",
    "        y = np.delete(y, subset_idx)    \n",
    "        worker_idx = worker_idx + n_workers\n",
    "\n",
    "    return worker_data, x, y\n",
    "\n",
    "if LABEL_DISTR_SKEW:\n",
    "    dataiter = label_distribution_skew(tensor_x, tensor_y, len(compute_nodes), skew=LABEL_DISTR_SKEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag and send split datasets to each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "if not LABEL_DISTR_SKEW:\n",
    "    trainloader = torch.utils.data.DataLoader(dataset_train, \n",
    "        # split data equally among nodes with shuffle\n",
    "        batch_size=dataset_train.__len__()//len(compute_nodes),\n",
    "        shuffle=True,\n",
    "        drop_last=True,)\n",
    "        #pin_memory=True) for faster dataloading to CUDA\n",
    "else: \n",
    "        trainloader = torch.utils.data.DataLoader(dataset_train, \n",
    "        # split data equally among nodes without shuffle\n",
    "        batch_size=dataset_train.__len__()//len(compute_nodes),\n",
    "        shuffle=False,\n",
    "        drop_last=True,)\n",
    "        #pin_memory=True) for faster dataloading to CUDA\n",
    "dataiter = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([155, 4, 80, 11]) torch.Size([155, 4, 1])\ntorch.Size([155, 4, 80, 11]) torch.Size([155, 4, 1])\n"
    }
   ],
   "source": [
    "shared_x = []\n",
    "shared_y = []\n",
    "for node in compute_nodes:\n",
    "    # create minibatches\n",
    "    worker_batch = dataiter.next()\n",
    "    sensors_train_tfan, labels_train_tfan = tuple_batch(worker_batch, MINIBATCH_SIZE)\n",
    "    print(sensors_train_tfan.shape, labels_train_tfan.shape)\n",
    "    # Tag tensors (allows them to be retrieved later)\n",
    "    if not DP_TYPE:\n",
    "        tagged_sensors = sensors_train_tfan.tag(\"#X\", \"#turbofan\", \"#dataset\").describe(\"The input datapoints to the Turbofan dataset.\")\n",
    "    elif DP_TYPE=='local':\n",
    "        tagged_sensors = sensors_train_tfan.tag(\"#X\", \"#localdp\", \"#turbofan\", \"#dataset\").describe(\"The input datapoints to the Turbofan dataset.\")\n",
    "    tagged_label = labels_train_tfan.tag(\"#Y\", \"#turbofan\", \"#dataset\").describe(\"The input labels to the Turbofan dataset.\")\n",
    "    \n",
    "    shared_x.append(tagged_sensors.send(node))\n",
    "    shared_y.append(tagged_label.send(node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X tensor pointers:  [(Wrapper)>[PointerTensor | me:67070848708 -> alice:7948937110]\n\tTags: #turbofan #dataset #localdp #X \n\tShape: torch.Size([155, 4, 80, 11])\n\tDescription: The input datapoints to the Turbofan dataset...., (Wrapper)>[PointerTensor | me:92409215119 -> bob:42608560662]\n\tTags: #turbofan #dataset #localdp #X \n\tShape: torch.Size([155, 4, 80, 11])\n\tDescription: The input datapoints to the Turbofan dataset....]\nY tensor pointers:  [(Wrapper)>[PointerTensor | me:66967328021 -> alice:66802965416]\n\tTags: #Y #dataset #turbofan \n\tShape: torch.Size([155, 4, 1])\n\tDescription: The input labels to the Turbofan dataset...., (Wrapper)>[PointerTensor | me:95935075564 -> bob:25910322289]\n\tTags: #Y #dataset #turbofan \n\tShape: torch.Size([155, 4, 1])\n\tDescription: The input labels to the Turbofan dataset....]\n"
    }
   ],
   "source": [
    "# print(\"X tensor pointers: \", shared_x1, shared_x2)\n",
    "# print(\"Y tensor pointers: \", shared_y1, shared_y2)\n",
    "\n",
    "print(\"X tensor pointers: \", shared_x)\n",
    "print(\"Y tensor pointers: \", shared_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disconnect nodes\n",
    "\n",
    "To ensure that our training process (in the Part 2 notebook), if located on the same server, is not using cached or local data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in compute_nodes:\n",
    "    node.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('pysyft27': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596528478357"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}