{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596852410168",
   "display_name": "Python 3.7.7 64-bit ('syft': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turbofan POC: Testing\n",
    "CAA 23/07/2020\n",
    "\n",
    "This notebook follows Part 1 and Part 2. Part 1 set up the grid infrastructure and populated the nodes with data. Part 2 trains a model.\n",
    "\n",
    "In this notebook, we will run a test script. You should be able to run this notebook on any server which is running a PyGridNetwork, or PyGridNode associated with the PyGridNetwork. The server running this notebook should have the validation dataset. It can be thought of as a validation server.\n",
    "\n",
    "Notebook dependencies:\n",
    "- [OpenMined Turbofan POC](https://github.com/matthiaslau/Turbofan-Federated-Learning-POC) repository (follow instructions for downloading and preprocessing the dataset, and place this notebook in the root directory of the repository)\n",
    "- PySyft 0.2.7\n",
    "\n",
    "NOTE: At the time of running this notebook, we were running the following processes.\n",
    "- PyGridNetwork: server Bob (http://localhost:5000)\n",
    "- PyGridNode: server Bob (http://localhost:3000)\n",
    "- PyGridNode: server Alice (http://18.218.13.132:3001)\n",
    "- This Jupyter Notebook: server Bob (http://localhost:8000)—you should be able to run this notebook on any server which is running a PyGridNetwork, or PyGridNode associated with the PyGridNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.clients.dynamic_fl_client import DynamicFLClient\n",
    "import torch\n",
    "import pandas as pd\n",
    "from numpy import mean, sqrt\n",
    "from numpy.random import laplace\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import *\n",
    "\n",
    "from turbofanpoc.federated_trainer.helper.data_helper import _load_data, WINDOW_SIZE, _drop_unnecessary_columns, _transform_to_windowed_data, get_data_loader, _clip_rul\n",
    "\n",
    "import models\n",
    "\n",
    "def add_rul_to_train_data(train_data):\n",
    "    \"\"\" Calculate and add the RUL to all rows in the given training data.\n",
    "\n",
    "    :param train_data: The training data\n",
    "    :return: The training data with added RULs\n",
    "    \"\"\"\n",
    "    # retrieve the max cycles per engine_node: RUL\n",
    "    train_rul = pd.DataFrame(train_data.groupby('engine_no')['time_in_cycles'].max()).reset_index()\n",
    "\n",
    "    # merge the RULs into the training data\n",
    "    train_rul.columns = ['engine_no', 'max']\n",
    "    train_data = train_data.merge(train_rul, on=['engine_no'], how='left')\n",
    "\n",
    "    # add the current RUL for every cycle\n",
    "    train_data['RUL'] = train_data['max'] - train_data['time_in_cycles']\n",
    "    train_data.drop('max', axis=1, inplace=True)\n",
    "\n",
    "    return train_data\n",
    "\n",
    "def batch(tensor, batch_size):\n",
    "    feature_shape = tensor.shape[1:]\n",
    "    return tensor.view(-1, batch_size, *feature_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"turbofanpoc/data\"\n",
    "TEST_DATA_NAME = \"test_data_test.txt\"\n",
    "MINIBATCH_SIZE = 4\n",
    "DP_TYPE = 'local'\n",
    "NOISE = 0.2\n",
    "MODEL_NAME = \"bnormfc\"\n",
    "WEIGHTS_SAVE_NAME = 'turbofan_80_bnormfc_None_None'\n",
    "WEIGHTS_DIR = './saved_weights'\n",
    "TRAIN_COLS = 11\n",
    "\n",
    "name2model = {\n",
    "    \"bnormfc\": BatchNormFCModel(WINDOW_SIZE, TRAIN_COLS), # modified to use batchnorm for normalisation\n",
    "    \"vanillalstm\": VanillaLSTM(WINDOW_SIZE, TRAIN_COLS),\n",
    "    \"vanillagru\": VanillaGRU(WINDOW_SIZE, TRAIN_COLS),\n",
    "}\n",
    "\n",
    "model_path = (Path(WEIGHTS_DIR) / WEIGHTS_SAVE_NAME).with_suffix('.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3305 features with shape (80, 11)\n3305 labels with shape (3305, 1)\n"
    }
   ],
   "source": [
    "data = _load_data(TEST_DATA_NAME, DATA_PATH)\n",
    "data_dropcol = _drop_unnecessary_columns(data)\n",
    "data_rul = add_rul_to_train_data(data_dropcol)\n",
    "x, y = _transform_to_windowed_data(data_rul, WINDOW_SIZE)\n",
    "y = _clip_rul(y)\n",
    " # transform to torch tensor\n",
    "tensor_x = torch.Tensor(x)\n",
    "tensor_y = torch.Tensor(y)\n",
    "\n",
    "dataset_test = torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "testloader = torch.utils.data.DataLoader(dataset_test, \n",
    "    # split data equally among nodes with shuffle\n",
    "    batch_size=MINIBATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,)\n",
    "    #pin_memory=True) for faster dataloading to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 826/826 [00:01<00:00, 737.79it/s]Begin testing...\nMean Root SSE: 29.020158767700195\n\n"
    }
   ],
   "source": [
    "# init model\n",
    "model = name2model[MODEL_NAME]\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "rtsse = []\n",
    "\n",
    "for data in tqdm(testloader):\n",
    "    # predict\n",
    "    sensors, labels = data\n",
    "    preds = model(sensors)\n",
    "    for i in range(MINIBATCH_SIZE):\n",
    "        label = labels[i]\n",
    "        rtsse.append(sqrt((preds - labels[i]).detach().numpy() ** 2))\n",
    "\n",
    "print('Begin testing...')\n",
    "print(f'Mean Root SSE: {mean(rtsse)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}