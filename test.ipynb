{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595465124519",
   "display_name": "Python 3.7.7 64-bit ('syft': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turbofan POC: Testing\n",
    "CAA 23/07/2020\n",
    "\n",
    "This notebook follows Part 1 and Part 2. Part 1 set up the grid infrastructure and populated the nodes with data. Part 2 trains a model.\n",
    "\n",
    "In this notebook, we will run a test script. You should be able to run this notebook on any server which is running a PyGridNetwork, or PyGridNode associated with the PyGridNetwork. The server running this notebook should have the validation dataset. It can be thought of as a validation server.\n",
    "\n",
    "Notebook dependencies:\n",
    "- [OpenMined Turbofan POC](https://github.com/matthiaslau/Turbofan-Federated-Learning-POC) repository (follow instructions for downloading and preprocessing the dataset, and place this notebook in the root directory of the repository)\n",
    "- PySyft 0.2.7\n",
    "\n",
    "NOTE: At the time of running this notebook, we were running the following processes.\n",
    "- PyGridNetwork: server Bob (http://localhost:5000)\n",
    "- PyGridNode: server Bob (http://localhost:3000)\n",
    "- PyGridNode: server Alice (http://18.218.13.132:3001)\n",
    "- This Jupyter Notebook: server Bob (http://localhost:8000)â€”you should be able to run this notebook on any server which is running a PyGridNetwork, or PyGridNode associated with the PyGridNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.clients.dynamic_fl_client import DynamicFLClient\n",
    "import torch\n",
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from numpy.random import laplace\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from federated_trainer.helper.data_helper import _load_data, WINDOW_SIZE, _drop_unnecessary_columns, _transform_to_windowed_data, get_data_loader, _clip_rul\n",
    "\n",
    "import models\n",
    "\n",
    "def add_rul_to_train_data(train_data):\n",
    "    \"\"\" Calculate and add the RUL to all rows in the given training data.\n",
    "\n",
    "    :param train_data: The training data\n",
    "    :return: The training data with added RULs\n",
    "    \"\"\"\n",
    "    # retrieve the max cycles per engine_node: RUL\n",
    "    train_rul = pd.DataFrame(train_data.groupby('engine_no')['time_in_cycles'].max()).reset_index()\n",
    "\n",
    "    # merge the RULs into the training data\n",
    "    train_rul.columns = ['engine_no', 'max']\n",
    "    train_data = train_data.merge(train_rul, on=['engine_no'], how='left')\n",
    "\n",
    "    # add the current RUL for every cycle\n",
    "    train_data['RUL'] = train_data['max'] - train_data['time_in_cycles']\n",
    "    train_data.drop('max', axis=1, inplace=True)\n",
    "\n",
    "    return train_data\n",
    "\n",
    "def batch(tensor, batch_size):\n",
    "    feature_shape = tensor.shape[1:]\n",
    "    return tensor.view(-1, batch_size, *feature_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data\"\n",
    "TEST_DATA_NAME = \"test_data_test.txt\"\n",
    "MINIBATCH_SIZE = 4\n",
    "DP_TYPE = 'local'\n",
    "NOISE = 0.2\n",
    "MODEL_NAME = 'turbofan_100'\n",
    "MODEL_PATH = './ihpc_models'\n",
    "TRAIN_COLS = 11\n",
    "\n",
    "model_path = Path(MODEL_PATH) / MODEL_NAME\n",
    "\n",
    "def laplacian_mechanism(input_tensor, sensitivity=0.5, epsilon=0.05):\n",
    "    '''\n",
    "    sensitivity and epsilon are arbitrarily \n",
    "    chosen for now\n",
    "    '''\n",
    "    beta = sensitivity / epsilon\n",
    "    noise = torch.tensor(laplace(0, beta, 1))\n",
    "    return input_tensor + noise\n",
    "\n",
    "def add_noise(input_tensor, p_noise):\n",
    "    '''\n",
    "    tensor: input tensor\n",
    "    p_noise: probability with which noise is added\n",
    "    '''\n",
    "    be_honest = (torch.rand(input_tensor.shape) < p_noise).float()\n",
    "    tensor_artificial = laplacian_mechanism(input_tensor)\n",
    "    # add noise\n",
    "    mod_tensor = input_tensor.float() * be_honest + (1 - be_honest) * tensor_artificial\n",
    "    sk_tensor = mod_tensor.float().mean()\n",
    "    # de-skew result\n",
    "    noisy_tensor = ((mod_tensor / p_noise) - 0.5) * p_noise / (1 - p_noise)\n",
    "    return mod_tensor.type(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2765 features with shape (80, 11)\n2765 labels with shape (2765, 1)\n"
    }
   ],
   "source": [
    "data = _load_data(TEST_DATA_NAME, DATA_PATH)\n",
    "data_dropcol = _drop_unnecessary_columns(data)\n",
    "data_rul = add_rul_to_train_data(data_dropcol)\n",
    "x, y = _transform_to_windowed_data(data_rul, WINDOW_SIZE)\n",
    "y = _clip_rul(y)\n",
    " # transform to torch tensor\n",
    "tensor_x = torch.Tensor(x)\n",
    "tensor_y = torch.Tensor(y)\n",
    "\n",
    "dataset_test = torch.utils.data.TensorDataset(tensor_x, tensor_y)\n",
    "testloader = torch.utils.data.DataLoader(dataset_test, \n",
    "    # split data equally among nodes with shuffle\n",
    "    batch_size=MINIBATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,)\n",
    "    #pin_memory=True) for faster dataloading to CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'features_size'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5da9ad045d26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormFCModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_COLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'features_size'"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "model = BatchNormFCModel(WINDOW_SIZE, TRAIN_COLS)\n",
    "model.load_state_dict(model_path)\n",
    "\n",
    "sse = []\n",
    "\n",
    "for data in tqdm(testloader):\n",
    "    # predict\n",
    "    sensors, labels = data\n",
    "    preds = model(sensors)\n",
    "    if DP_TYPE=='global':\n",
    "        preds = add_noise(preds, NOISE)\n",
    "    for i in range(MINIBATCH_SIZE):\n",
    "        label = labels[i]\n",
    "        sse.append((preds - labels[i]) ** 2)\n",
    "\n",
    "print(f\"Mean SSE: {mean(sse)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}