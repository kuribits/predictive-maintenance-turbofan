{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turbofan POC: Training\n",
    "CAA 23/07/2020\n",
    "\n",
    "This notebook follows Part 1 which set up the grid infrastructure, and populated the nodes with data.\n",
    "\n",
    "In this notebook, we will run the training. You should be able to run this notebook on any server which is running a PyGridNetwork, or PyGridNode associated with the PyGridNetwork. \n",
    "\n",
    "NOTE: This notebook requires that instances associated with nodes have been sent data using PySyft's .send() method. Additionally, at the time of running this notebook, we were running the following processes.\n",
    "- PyGridNetwork: server Bob (http://localhost:5000)\n",
    "- PyGridNode: server Bob (http://localhost:3000)\n",
    "- PyGridNode: server Alice (http://18.218.13.132:3001)\n",
    "- This Jupyter Notebook: server Bob (http://localhost:8000)—you should be able to run this notebook on any server which is running a PyGridNetwork, or PyGridNode associated with the PyGridNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.grid.public_grid import PublicGridNetwork\n",
    "import torch as torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "from statistics import mean\n",
    "from pathlib import Path\n",
    "\n",
    "from turbofanpoc.federated_trainer.helper.trainings_helper import data_result_size, start_federated_training, history\n",
    "from turbofanpoc.federated_trainer.helper.trainings_helper import get_model_error\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up network & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "WARNING:root:Torch was already hooked... skipping hooking process\n"
    }
   ],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if(torch.cuda.is_available()):\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up train configs and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grid config\n",
    "MODEL_NAME = \"fc\"\n",
    "GRID_ADDRESS = 'localhost'\n",
    "GRID_PORT = '5000'\n",
    "# Model training config\n",
    "DATA_TAGS = (\"#X\", \"#turbofan\", \"#dataset\")\n",
    "LABEL_TAGS = (\"#Y\", \"#turbofan\", \"#dataset\")\n",
    "WEIGHTS_NAME = \"BNFC_datanoise_0.2\"\n",
    "# MODEL_ID = \"turbofan\"\n",
    "SAVE_MODEL = True\n",
    "WEIGHTS_DIR = './saved_weights'\n",
    "TRAIN_COLS = 11\n",
    "WINDOW_SIZE = 80\n",
    "MAX_EPOCHS = 100 # used by Turbofan demo authors\n",
    "LOAD_MODEL = True\n",
    "METRICS_INTERVAL = 10\n",
    "AGGREGATION = 'weight' #{weight, gradients}\n",
    "# Differential privacy config\n",
    "DP_TYPE = 'local' #{local, global, layer-wise}\n",
    "\n",
    "def save_model(model, training_rounds, id=\"\"):\n",
    "    if not Path(WEIGHTS_DIR).exists():\n",
    "        Path(WEIGHTS_DIR).mkdir(parents=True)\n",
    "    \"\"\" Save a torch model to disk.\n",
    "\n",
    "    :param model: Model to save\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), \"{}/turbofan_{}_{}.pt\".format(WEIGHTS_DIR, training_rounds, id))\n",
    "\n",
    "\n",
    "def load_initial_model():\n",
    "    \"\"\" Load the model from the initial training from disk.\n",
    "\n",
    "    :return: The initial model\n",
    "    \"\"\"\n",
    "    return torch.load(\"{}/turbofan_initial.pt\".format(WEIGHTS_DIR))\n",
    "\n",
    "\n",
    "def load_latest_model():\n",
    "    \"\"\" Load the latest model created during federated learning from disk.\n",
    "\n",
    "    :return: The latest model\n",
    "    \"\"\"\n",
    "    index = training_rounds - 1\n",
    "    if index == 0:\n",
    "        index = \"initial\"\n",
    "    return torch.load(\"{}/turbofan_{}.pt\".format(WEIGHTS_DIR, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search grid for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfan_grid = PublicGridNetwork(hook,\"http://\" + GRID_ADDRESS + \":\" + GRID_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DP_TYPE:\n",
    "    data = tfan_grid.search(*DATA_TAGS)\n",
    "# for data that has undergone local dp\n",
    "elif DP_TYPE=='local':\n",
    "    data = tfan_grid.search(*DATA_TAGS, \"#localdp\")\n",
    "else: raise NotImplementedError\n",
    "target = tfan_grid.search(*LABEL_TAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(data.values())\n",
    "target = list(target.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(Wrapper)>[PointerTensor | me:91448401662 -> bob:99366173661]\n\tTags: #turbofan #dataset #localdp #X \n\tShape: torch.Size([155, 4, 80, 11])\n\tDescription: The input datapoints to the Turbofan dataset....], [(Wrapper)>[PointerTensor | me:44250389676 -> alice:94445306550]\n\tTags: #localdp #dataset #X #turbofan \n\tShape: torch.Size([155, 4, 80, 11])\n\tDescription: The input datapoints to the Turbofan dataset....]]\n"
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[(Wrapper)>[PointerTensor | me:14654811455 -> bob:26279338334]\n\tTags: #Y #dataset #turbofan \n\tShape: torch.Size([155, 4, 1])\n\tDescription: The input labels to the Turbofan dataset....], [(Wrapper)>[PointerTensor | me:69085487007 -> alice:78518394709]\n\tTags: #turbofan #dataset #Y \n\tShape: torch.Size([155, 4, 1])\n\tDescription: The input labels to the Turbofan dataset....]]\n"
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading initial model...\nBeginning training...\nTrain epoch: 0\t| Worker: bob\t| [0/100 (0%)] | \tLoss: 26.608482\t| Time: 10.525667s\nTrain epoch: 0\t| Worker: alice\t| [0/100 (0%)] | \tLoss: 27.520130\t| Time: 10.056374s\nTrain epoch: 0\t complete| Time: 20.584469s | Mean iteration time: 0.016175\nTrain epoch: 10\t| Worker: bob\t| [10/100 (10%)] | \tLoss: 22.749996\t| Time: 11.399249s\nTrain epoch: 10\t| Worker: alice\t| [10/100 (10%)] | \tLoss: 27.500000\t| Time: 10.098290s\nTrain epoch: 10\t complete| Time: 21.500218s | Mean iteration time: 0.016243\nTrain epoch: 20\t| Worker: bob\t| [20/100 (20%)] | \tLoss: 22.750002\t| Time: 10.203395s\nTrain epoch: 20\t| Worker: alice\t| [20/100 (20%)] | \tLoss: 27.500000\t| Time: 11.322445s\nTrain epoch: 20\t complete| Time: 21.528477s | Mean iteration time: 0.018213\nTrain epoch: 30\t| Worker: bob\t| [30/100 (30%)] | \tLoss: 22.750000\t| Time: 11.235489s\nTrain epoch: 30\t| Worker: alice\t| [30/100 (30%)] | \tLoss: 27.500000\t| Time: 10.279230s\nTrain epoch: 30\t complete| Time: 21.517339s | Mean iteration time: 0.016535\nTrain epoch: 40\t| Worker: bob\t| [40/100 (40%)] | \tLoss: 22.750000\t| Time: 10.254935s\nTrain epoch: 40\t| Worker: alice\t| [40/100 (40%)] | \tLoss: 27.500000\t| Time: 10.448678s\nTrain epoch: 40\t complete| Time: 20.706069s | Mean iteration time: 0.016808\nTrain epoch: 50\t| Worker: bob\t| [50/100 (50%)] | \tLoss: 22.749998\t| Time: 11.357126s\nTrain epoch: 50\t| Worker: alice\t| [50/100 (50%)] | \tLoss: 27.499998\t| Time: 10.335269s\nTrain epoch: 50\t complete| Time: 21.694795s | Mean iteration time: 0.016625\nTrain epoch: 60\t| Worker: bob\t| [60/100 (60%)] | \tLoss: 22.750000\t| Time: 10.112203s\nTrain epoch: 60\t| Worker: alice\t| [60/100 (60%)] | \tLoss: 27.499998\t| Time: 10.262091s\nTrain epoch: 60\t complete| Time: 20.376769s | Mean iteration time: 0.016508\nTrain epoch: 70\t| Worker: bob\t| [70/100 (70%)] | \tLoss: 22.750000\t| Time: 10.546638s\nTrain epoch: 70\t| Worker: alice\t| [70/100 (70%)] | \tLoss: 27.500000\t| Time: 11.606202s\nTrain epoch: 70\t complete| Time: 22.155184s | Mean iteration time: 0.018675\nTrain epoch: 80\t| Worker: bob\t| [80/100 (80%)] | \tLoss: 22.749998\t| Time: 10.188200s\nTrain epoch: 80\t| Worker: alice\t| [80/100 (80%)] | \tLoss: 28.443817\t| Time: 10.121156s\nTrain epoch: 80\t complete| Time: 20.311845s | Mean iteration time: 0.016280\nTrain epoch: 90\t| Worker: bob\t| [90/100 (90%)] | \tLoss: 22.750002\t| Time: 10.805295s\nTrain epoch: 90\t| Worker: alice\t| [90/100 (90%)] | \tLoss: 28.434643\t| Time: 10.813971s\nTrain epoch: 90\t complete| Time: 21.621624s | Mean iteration time: 0.017373\nSaving model trained with 100 epochs at ./saved_weights...\n"
    }
   ],
   "source": [
    "# initialise model\n",
    "name2model = {\n",
    "    \"mbfc\": BatchFCModel(), # modified to use mean of minibatch for normalisation\n",
    "    \"bnormfc\": BatchNormFCModel(WINDOW_SIZE, TRAIN_COLS), # modified to use batchnorm for normalisation\n",
    "}\n",
    "\n",
    "model = name2model[MODEL_NAME]\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = nn.L1Loss() # used by Turbofan demo authors\n",
    "# criterion = nn.CrossEntropyLoss()                  \n",
    "if LOAD_MODEL:\n",
    "    try:\n",
    "        print(\"Loading initial model...\")\n",
    "        model = load_initial_model()\n",
    "        model.to_device\n",
    "        print(\"Done.\")\n",
    "    except: \"No initial model found\"\n",
    "\n",
    "def train(max_epochs):\n",
    "    model.train()\n",
    "    print(\"Beginning training...\")\n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_start_t = time()\n",
    "        for i in range(len(data)):\n",
    "            # loop over workers\n",
    "            worker_start_t = time()\n",
    "            for j in range(len(data[i])):\n",
    "                # loop over batches\n",
    "                worker = data[i][j].location\n",
    "                model.send(worker)\n",
    "                it_ts = []\n",
    "                for k in range(len(data[i][j])):\n",
    "                    mb_start_t = time()\n",
    "                    # loop over minibatches\n",
    "                    mb_data = data[i][j][k]\n",
    "                    mb_target = target[i][j][k]\n",
    "                    optimizer.zero_grad()\n",
    "                    pred = model(mb_data)\n",
    "                    loss = criterion(pred, mb_target)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    it_ts.append(time()-mb_start_t)\n",
    "                model.get()\n",
    "                loss = loss.get()\n",
    "\n",
    "            worker_t = time()-worker_start_t\n",
    "            if epoch % METRICS_INTERVAL==0 or epoch == MAX_EPOCHS:\n",
    "                print('Train epoch: {}\\t| Worker: {}\\t| [{}/{} ({:.0f}%)] | \\tLoss: {:.6f}\\t| Time: {:.6f}s'.format(epoch, worker.id, epoch, MAX_EPOCHS, 100. *  epoch / MAX_EPOCHS, loss.item(), worker_t)) \n",
    "        mean_it_t = mean(it_ts)/len(data[0][0][0])\n",
    "        if epoch % METRICS_INTERVAL==0:\n",
    "            epoch_t = time()-epoch_start_t\n",
    "            print(f'Train epoch: {epoch}\\t complete| Time: {epoch_t:.6f}s | Mean iteration time: {mean_it_t:.6f}')\n",
    "    \n",
    "train(MAX_EPOCHS)\n",
    "\n",
    "if SAVE_MODEL==True:\n",
    "    print(f\"Saving model trained with {MAX_EPOCHS} epochs at {WEIGHTS_DIR}...\")\n",
    "    save_model(model, MAX_EPOCHS, WEIGHTS_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('syft': conda)",
   "language": "python",
   "name": "python_defaultSpec_1596529633197"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}